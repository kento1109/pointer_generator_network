{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformersライブラリでT5を試す\n",
    "\n",
    "ref. https://medium.com/askdata/train-t5-for-text-summarization-a1926f52d281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語モデルをロード  \n",
    "https://huggingface.co/sonoisa/t5-base-japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "  \n",
    "tokenizer = T5Tokenizer.from_pretrained(\"sonoisa/t5-base-japanese\")\n",
    "# model = AutoModelWithLMHead.from_pretrained(\"sonoisa/t5-base-japanese\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('sonoisa/t5-base-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁6 日 、 荒川 静 香 が 東京 ミッド タウン の スケート リンク の セレモニー に出席し た 。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.tokenize('6日、荒川静香が東京ミッドタウンのスケートリンクのセレモニーに出席した。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ad8ccb2b621f9de2\n",
      "Reusing dataset csv (/home/sugimoto/.cache/huggingface/datasets/csv/default-ad8ccb2b621f9de2/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': ['train.csv'], 'test': 'test.csv'})\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " '新宿',\n",
       " '駅構内',\n",
       " 'で',\n",
       " '31',\n",
       " '日まで',\n",
       " '、「',\n",
       " '新宿',\n",
       " 'ドラゴンクエスト',\n",
       " 'ジャック',\n",
       " '」',\n",
       " 'が開催される',\n",
       " '。',\n",
       " '「',\n",
       " 'ブロック',\n",
       " 'モンスター',\n",
       " '討伐',\n",
       " '作戦',\n",
       " '」「',\n",
       " '新宿',\n",
       " 'モンスター',\n",
       " 'ロード',\n",
       " '」',\n",
       " 'の',\n",
       " '2',\n",
       " 'つ',\n",
       " 'を',\n",
       " '楽しめる',\n",
       " '。',\n",
       " '初日',\n",
       " 'は約',\n",
       " '18',\n",
       " '万',\n",
       " '個の',\n",
       " 'ブロック',\n",
       " 'で',\n",
       " 'ドット',\n",
       " '状に',\n",
       " '描いた',\n",
       " 'モンスター',\n",
       " 'が',\n",
       " '出現する',\n",
       " 'そう']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(test_dataset[0]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def tokenize(batch):\n",
    "    text = list(map(preprocess,  batch['text']))\n",
    "    summary = list(map(preprocess, batch['summary']))\n",
    "    # 大文字を小文字に変換しておく\n",
    "    tokenized_input = tokenizer(text, padding='max_length', truncation=True, max_length=500)\n",
    "    tokenized_label = tokenizer(summary, padding='max_length', truncation=True, max_length=100)\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(['暑い夏の日'], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sugimoto/.cache/huggingface/datasets/csv/default-ad8ccb2b621f9de2/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-cfd154398e4c6ed0.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
    "# test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.LongTensor(train_dataset['input_ids'][:5]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_ids = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_tokens = tokenizer.convert_ids_to_tokens(decoded_ids[0])\n",
    "eos_idx = decoded_tokens.index('</s>')\n",
    "# decoded_tokens[1: eos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the mor vRAM used)\n",
    "    prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
    "    learning_rate=0.001,\n",
    "    evaluation_strategy='steps', # Run evaluation every eval_steps\n",
    "    save_steps=1000, # How often to save a checkpoint\n",
    "    save_total_limit=1, # Number of maximum checkpoints to save\n",
    "    remove_unused_columns=True, # Removes useless columns from the dataset\n",
    "    run_name='run_name', # Wandb run name\n",
    "    logging_steps=1000, # How often to log loss to wandb\n",
    "    eval_steps=1000, # How often to run evaluation on the val_set\n",
    "    logging_first_step=False, # Whether to log also the very first training step to wandb\n",
    "    load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n",
    "    metric_for_best_model=\"loss\", # Use loss to evaluate best model.\n",
    "    greater_is_better=False # Best model is the one with the lowest loss, not highest.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [470/470 11:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=470, training_loss=2.475035062749335)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"任天堂の据え置き型ゲーム機「wii u」に年内生産終了報道が出ている。これは3月23日付けの日経新聞 電子版が伝えているもの。「wii u」は発売からまだ3年強だが、「（任天堂は）販売が振るわず、回復が見込めないと判断した」（同紙より）という。「wii u」を巡ってはここ最近、「スーパーマリオメーカー」や「splatoon（スプラトゥーン）」のヒットや、世界的なヒット作「マインクラフト」の投入などにより「本体が欲しい」とのニーズが高まる一方、ネットでは「お店に売っていない」「どこで買えるの？」など品薄状態への不満の声が上がっていた。\"\n",
    "inputs = tokenizer.encode(text, padding='max_length', return_tensors=\"pt\", max_length=500, truncation=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = '任天堂の据え置き型ゲーム機「Wii U」に年内生産終了報道が出ている。「（任天堂は）販売が振るわず、回復が見込めないと判断した」という。一方で「どこで買えるの？」など品薄状態への不満の声も上がっていた'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 任天堂の据え置き型ゲーム機「wii u」に年内生産終了が報じられた。「お店に売っていない」「どこで買えるの?」などの不満の声が上がっていたそう。ネットでは「どこで買えるの?」など品薄状態への不満の声が上がっていた</s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# model.eval()\n",
    "with torch.no_grad():\n",
    "    summary_ids = model.generate(inputs) #, max_length=512, min_length=5, length_penalty=5., num_beams=2)\n",
    "    summary = tokenizer.decode(summary_ids[0])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizationにおいてUNKが多く発生する  \n",
    "特に大文字英単語はほとんどカバーできていない印象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', '▁ii', '▁', 'U']\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('Wii U'))\n",
    "print('W' in tokenizer.get_vocab())\n",
    "print('Ｗ' in tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'spiece.model'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(os.path.join(tokenizer.vocab_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '任天堂の据え置き型ゲーム機「Wii U」に年内生産終了報道が出ている。'\n",
    "tokens = sp.EncodeAsPieces(text)\n",
    "# print(tokenizer.tokenize(text))\n",
    "token_ids = sp.EncodeAsIds(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'任天堂の据え置き型ゲーム機「 ⁇ ii  ⁇ 」に年内生産終了報道が出ている。'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2, 25, 396, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp['W'], sp['Ｗ'], sp['３'], sp['3'], sp['w'], sp['ｗ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.IdToPiece(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for '/home/sugimoto/japanese_bart_base_2.0'. Make sure that:\n\n- '/home/sugimoto/japanese_bart_base_2.0' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/home/sugimoto/japanese_bart_base_2.0' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e7d0b2e9678c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/sugimoto/japanese_bart_base_2.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m             )\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/home/sugimoto/japanese_bart_base_2.0'. Make sure that:\n\n- '/home/sugimoto/japanese_bart_base_2.0' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/home/sugimoto/japanese_bart_base_2.0' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/sugimoto/japanese_bart_base_2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'do_basic_tokenize': False} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"放 射線 治療\", do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
