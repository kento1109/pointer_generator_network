{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False, False,  True, False,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "token_ids = torch.Tensor([12, 1, 0, 3, 3, 0, 2, 0])\n",
    "token_ids == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_TOKEN = '。'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "SOS_TOKEN = '[SOS]'\n",
    "EOS_TOKEN = '[EOS]'\n",
    "SPECIAL_TOKENS = [UNK_TOKEN, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, vocab_path: str):\n",
    "        \"\"\" constructor \"\"\"\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(SPECIAL_TOKENS)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.build_vocab(vocab_path)\n",
    "\n",
    "    def read_vocab_file(self, vocab_path: str) -> List[str]:\n",
    "        vocab = list()\n",
    "        with open(vocab_path) as vocab_file:\n",
    "            for word in vocab_file:\n",
    "                vocab.append(word.strip())\n",
    "        return vocab\n",
    "\n",
    "    def build_vocab(self, vocab_path: str) -> None:\n",
    "        \"\"\" build vocabulary \"\"\"\n",
    "        vocab = self.read_vocab_file(vocab_path)\n",
    "        for word in vocab:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def get_oovs(self, tokens: List[str]) -> List[str]:\n",
    "        return list(filter(lambda token: token not in self.word2idx, tokens))\n",
    "\n",
    "    def get_mask(self, tokens: List[str]) -> List[int]:\n",
    "        return [1 if token != PAD_TOKEN else 0 for token in tokens]\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\" encode tokens \"\"\"\n",
    "        return [self.word2idx.get(token, self.word2idx[UNK_TOKEN]) for token in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        \"\"\" decode indices \"\"\"\n",
    "        return [self.idx2word[idx] for idx in ids]\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        \"\"\" constructor \"\"\"\n",
    "        self.mc = MeCab.Tagger('-Owakati')\n",
    "\n",
    "    def __call__(self, text: str, sos_token: bool = None, eos_token: bool = None, padding: bool = False, max_length: Optional[int] = None, truncation: bool = False) -> List[str]:\n",
    "        tokens = self.tokenize(text)\n",
    "        if sos_token is not None:\n",
    "            tokens = [sos_token] + tokens\n",
    "        if eos_token is not None:\n",
    "            tokens = tokens + [eos_token]\n",
    "        if padding and max_length is not None:\n",
    "            padding_length = max_length - len(tokens)\n",
    "            tokens += [PAD_TOKEN] * padding_length\n",
    "        if truncation and max_length is not None:\n",
    "            # if the length of tokens is over max_length, the eos token is truncated\n",
    "            tokens = tokens[:max_length]\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\" simple tokenization \"\"\"\n",
    "        return self.mc.parse(text).strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "src_vocab = Vocab('data/source_vocab.txt')\n",
    "tgt_vocab = Vocab('data/target_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_MAX_LENGTH = 500\n",
    "TGT_MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class BaseExample():\n",
    "    def __init__(self, input_ids: List[int], mask: List[int]):\n",
    "        self.input_ids = input_ids\n",
    "        self.mask = mask\n",
    "\n",
    "class EncExample(BaseExample):\n",
    "    def __init__(self, input_ids: List[int], extended_ids: List[int], mask: List[int], n_oovs: int):\n",
    "        super(EncExample, self).__init__(input_ids, mask)\n",
    "        self.extended_ids = extended_ids\n",
    "        self.n_oovs = n_oovs\n",
    "\n",
    "class DecExample(BaseExample):\n",
    "    def __init__(self, input_ids: List[int], target_ids: List[int], mask: List[int]):\n",
    "        super(DecExample, self).__init__(input_ids, mask)\n",
    "        self.target_ids = target_ids\n",
    "\n",
    "class SummExample():\n",
    "    def __init__(self, enc_example: EncExample, dec_example: DecExample):\n",
    "        self.enc = enc_example\n",
    "        self.dec = dec_example  \n",
    "\n",
    "class BaseBatch():\n",
    "    def __init__(self, input_ids: torch.Tensor, mask: torch.Tensor):\n",
    "        self.input_ids = input_ids\n",
    "        self.mask = mask \n",
    "\n",
    "class EncBatch(BaseBatch):\n",
    "    def __init__(self, input_ids: torch.Tensor, extended_ids: torch.Tensor, mask: torch.Tensor, max_n_oovs: int):\n",
    "        super(EncBatch, self).__init__(input_ids, mask)\n",
    "        self.input_ids = input_ids\n",
    "        self.extended_ids = extended_ids\n",
    "        self.mask = mask \n",
    "        self.max_n_oovs = max_n_oovs\n",
    "\n",
    "class DecBatch(BaseBatch):\n",
    "    def __init__(self, input_ids: torch.Tensor, target_ids: torch.Tensor, mask: torch.Tensor):\n",
    "        super(DecBatch, self).__init__(input_ids, mask)\n",
    "        self.input_ids = input_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.mask = mask \n",
    "\n",
    "class SummBatch():\n",
    "    def __init__(self, enc_batch: EncBatch, dec_batch: DecBatch):\n",
    "        self.enc = enc_batch\n",
    "        self.dec = dec_batch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[SOS]', '暑い', '夏', 'の', '日', 'に', 'は', 'アイス', 'を', '食べ', 'たく', 'なり', 'ます', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['暑い', '夏', 'の', '日', 'に', 'は', 'アイス', 'を', '食べ', 'たく', 'なり', 'ます', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('暑い夏の日にはアイスを食べたくなります', sos_token=SOS_TOKEN, truncation=True, max_length=20, padding=True))\n",
    "print(tokenizer('暑い夏の日にはアイスを食べたくなります', eos_token=EOS_TOKEN, truncation=True, max_length=20, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, tokenizer, src_vocab, tgt_vocab, config):\n",
    "        \"\"\" constructor \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.model.to(self.device)\n",
    "        self.config = config\n",
    "\n",
    "    def _prepare_enc_example(self, tokens: List[str], vocab: Vocab) -> EncExample:\n",
    "        def _replace_oovs() -> List[int]:\n",
    "            extend_ids = list()\n",
    "            oov_words = list()\n",
    "            vocab_size = vocab.size()\n",
    "            unk_idx = vocab.word2idx[UNK_TOKEN]\n",
    "            for token, input_id in zip(tokens, input_ids):\n",
    "                if input_id == unk_idx:\n",
    "                    if token not in oov_words:\n",
    "                        oov_words.append(token)\n",
    "                    n_oovs = oov_words.index(token)\n",
    "                    extend_ids.append(vocab_size + n_oovs)\n",
    "                else:\n",
    "                    extend_ids.append(input_id)\n",
    "            return extend_ids\n",
    "        input_ids = vocab.encode(tokens)\n",
    "        extended_ids = _replace_oovs()\n",
    "        mask = vocab.get_mask(tokens)\n",
    "        n_oovs = vocab.get_oovs(tokens)\n",
    "        return EncExample(input_ids, extended_ids, mask, len(n_oovs))\n",
    "\n",
    "    def _prepare_dec_example(self, input_tokens: List[str], target_tokens: List[str], vocab: Vocab, oov_words: List[str]) -> DecExample:\n",
    "        def _replace_oovs() -> List[int]:\n",
    "            target_ids_ = list()\n",
    "            vocab_size = vocab.size()\n",
    "            unk_idx = vocab.word2idx[UNK_TOKEN]\n",
    "            for target_token, target_id in zip(target_tokens, target_ids):\n",
    "                if target_id == unk_idx:\n",
    "                    if target_token in oov_words:\n",
    "                        target_ids_.append(vocab_size + oov_words.index(target_token))\n",
    "                    else:\n",
    "                        target_ids_.append(target_id)\n",
    "                else:\n",
    "                    target_ids_.append(target_id)\n",
    "            return target_ids_\n",
    "        input_ids = vocab.encode(input_tokens)\n",
    "        target_ids = vocab.encode(target_tokens)\n",
    "        target_ids = _replace_oovs()\n",
    "        mask = vocab.get_mask(input_tokens)\n",
    "        return DecExample(input_ids, target_ids, mask)        \n",
    "\n",
    "    def convert_article_to_example(self, article_dict: Dict[str, str]) -> SummExample:\n",
    "        \"\"\" convert article dicts to example \"\"\"\n",
    "        src_text, tgt_text = article_dict['body'], self.concat_summary(article_dict['summary'])\n",
    "        enc_input_tokens = self.tokenizer(src_text, padding=True, max_length=SRC_MAX_LENGTH, truncation=True)\n",
    "        oov_words = self.src_vocab.get_oovs(enc_input_tokens)\n",
    "        dec_input_tokens = self.tokenizer(tgt_text, sos_token=SOS_TOKEN, padding=True, max_length=TGT_MAX_LENGTH, truncation=True)\n",
    "        dec_target_tokens = self.tokenizer(tgt_text, eos_token=EOS_TOKEN, padding=True, max_length=TGT_MAX_LENGTH, truncation=True)\n",
    "        # store data as an example instance\n",
    "        enc_example = self._prepare_enc_example(enc_input_tokens, self.src_vocab)\n",
    "        dec_example = self._prepare_dec_example(dec_input_tokens, dec_target_tokens, self.tgt_vocab, oov_words)\n",
    "        example = SummExample(enc_example, dec_example)\n",
    "        return example\n",
    "\n",
    "    @staticmethod\n",
    "    def load_jsons(data_path_pattern: str) -> List[Dict[str, str]]:\n",
    "        \"\"\" load json files \"\"\"\n",
    "        json_paths = glob.glob(os.path.join(data_path_pattern))\n",
    "        article_dicts = list()\n",
    "        for json_path in json_paths:\n",
    "            with open(json_path) as json_file:\n",
    "                article_dicts.extend(json.load(json_file))\n",
    "        return article_dicts\n",
    "\n",
    "    @staticmethod\n",
    "    def concat_summary(summaries: List[str]) -> str:\n",
    "        \"\"\" concat summaries to a chunk \"\"\"\n",
    "        return SEP_TOKEN.join(summaries)\n",
    "\n",
    "    def build_data_loader(self, examples: List[SummExample], shuffle: bool = False) -> DataLoader:\n",
    "        \"\"\" build data loader from examples\"\"\"\n",
    "        return DataLoader(examples,\n",
    "                          batch_size=self.config.batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          collate_fn=self._collate_examples)\n",
    "\n",
    "    def _collate_examples(self, examples: List[SummExample]) -> SummBatch:\n",
    "        \"\"\" collate lists of samples into batch \"\"\"\n",
    "        enc_examples = list(map(lambda example : example.enc, examples))\n",
    "        enc_batch = self._enc_examples_to_batch(enc_examples)\n",
    "        dec_examples = list(map(lambda example : example.dec, examples))\n",
    "        dec_batch = self._dec_examples_to_batch(dec_examples)\n",
    "        return SummBatch(enc_batch, dec_batch)\n",
    "\n",
    "    @staticmethod \n",
    "    def _enc_examples_to_batch(examples: List[EncExample]) -> EncBatch:\n",
    "        \"\"\" convert encoder examples to tensor \"\"\"\n",
    "        enc_dict = dict()\n",
    "        enc_dict['input_ids'] = torch.tensor([example.input_ids for example in examples], dtype=torch.long)\n",
    "        enc_dict['extended_ids'] = torch.tensor([example.extended_ids for example in examples], dtype=torch.long)\n",
    "        enc_dict['mask'] = torch.tensor([example.mask for example in examples], dtype=torch.long)\n",
    "        # determine the max number of in-article OOVs in this batch\n",
    "        enc_dict['max_n_oovs'] = max(list(map(lambda example: example.n_oovs, examples)))\n",
    "        return EncBatch(**enc_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dec_examples_to_batch(examples: List[DecExample]) -> DecBatch:\n",
    "        \"\"\" convert decoder examples to tensor \"\"\"\n",
    "        dec_dict = dict()\n",
    "        dec_dict['input_ids'] = torch.tensor([example.input_ids for example in examples], dtype=torch.long)\n",
    "        dec_dict['target_ids'] = torch.tensor([example.target_ids for example in examples], dtype=torch.long)\n",
    "        dec_dict['mask'] = torch.tensor([example.mask for example in examples], dtype=torch.long)\n",
    "        return DecBatch(**dec_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_device(device, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\" transfrer data to device\"\"\"\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        return inputs\n",
    "\n",
    "class TrainerConfig():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "trainer_config = TrainerConfig(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer('model', tokenizer, src_vocab, tgt_vocab, trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_dict = {'body': '暑い夏の日にはアイスとニョロが食べたくなります。そんな日には冷えたジュースも良いでしょう。', 'summary':['暑い夏にはアイスとニョロとジュースが欲しい']}\n",
    "# example = trainer.convert_article_to_example(article_dict)\n",
    "# print(example.enc.input_ids)\n",
    "# print(example.enc.extended_ids)\n",
    "# print(example.dec.input_ids)\n",
    "# print(example.dec.target_ids)\n",
    "# print(tgt_vocab.decode(example.dec.input_ids))\n",
    "# print(tgt_vocab.decode(example.dec.target_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習（イテレーション）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.vocab_size = 50004\n",
    "        self.emb_dim = 128\n",
    "        self.hidden_dim = 256\n",
    "        self.pointer_gen = True\n",
    "        self.is_coverage = True\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(config.emb_dim,\n",
    "                            config.hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.W_h = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        outputs, hidden = self.lstm(embedded)\n",
    "        outputs = outputs.contiguous()\n",
    "        feature = outputs.view(-1, 2 * config.hidden_dim)  # B * t_k x 2*hidden_dim\n",
    "        feature = self.W_h(feature)\n",
    "\n",
    "        return outputs, feature, hidden\n",
    "\n",
    "class ReduceState(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReduceState, self).__init__()\n",
    "\n",
    "        self.reduce_h = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        self.reduce_c = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        h, c = hidden  # h, c dim = 2 x b x hidden_dim\n",
    "        h_in = h.transpose(0, 1).contiguous().view(-1, config.hidden_dim * 2)\n",
    "        hidden_reduced_h = F.relu(self.reduce_h(h_in))\n",
    "        c_in = c.transpose(0, 1).contiguous().view(-1, config.hidden_dim * 2)\n",
    "        hidden_reduced_c = F.relu(self.reduce_c(c_in))\n",
    "\n",
    "        return (hidden_reduced_h.unsqueeze(0), hidden_reduced_c.unsqueeze(0)\n",
    "                )  # h, c dim = 1 x b x hidden_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "enc = Encoder()\n",
    "reduce_state = ReduceState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_dicts = trainer.load_jsons('data/train1.json')\n",
    "# examples = list(map(trainer.convert_article_to_example, article_dicts))\n",
    "\n",
    "# data_loader = trainer.build_data_loader(examples)\n",
    "\n",
    "# for batch in data_loader:\n",
    "#     enc_outputs, enc_feature, enc_hidden = enc(batch.enc.input_ids)\n",
    "#     s_t_1 = reduce_state(enc_hidden)\n",
    "#     assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        # attention\n",
    "        if config.is_coverage:\n",
    "            self.W_c = nn.Linear(1, config.hidden_dim * 2, bias=False)\n",
    "        self.decode_proj = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2)\n",
    "        self.v = nn.Linear(config.hidden_dim * 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, s_t_hat, encoder_outputs, encoder_feature, enc_padding_mask, coverage):\n",
    "        b, t_k, n = list(encoder_outputs.size())\n",
    "\n",
    "        dec_fea = self.decode_proj(s_t_hat)  # B x 2*hidden_dim\n",
    "        dec_fea_expanded = dec_fea.unsqueeze(1).expand(b, t_k,\n",
    "                                                       n).contiguous()  # B x t_k x 2*hidden_dim\n",
    "        dec_fea_expanded = dec_fea_expanded.view(-1, n)  # B * t_k x 2*hidden_dim\n",
    "\n",
    "        att_features = encoder_feature + dec_fea_expanded  # B * t_k x 2*hidden_dim\n",
    "        if config.is_coverage:\n",
    "            coverage_input = coverage.view(-1, 1)  # B * t_k x 1\n",
    "            coverage_feature = self.W_c(coverage_input)  # B * t_k x 2*hidden_dim\n",
    "            att_features = att_features + coverage_feature\n",
    "\n",
    "        e = F.tanh(att_features)  # B * t_k x 2*hidden_dim\n",
    "        scores = self.v(e)  # B * t_k x 1\n",
    "        scores = scores.view(-1, t_k)  # B x t_k\n",
    "\n",
    "        attn_dist_ = F.softmax(scores, dim=1) * enc_padding_mask  # B x t_k\n",
    "        normalization_factor = attn_dist_.sum(1, keepdim=True)\n",
    "        attn_dist = attn_dist_ / normalization_factor\n",
    "\n",
    "        attn_dist = attn_dist.unsqueeze(1)  # B x 1 x t_k\n",
    "        c_t = torch.bmm(attn_dist, encoder_outputs)  # B x 1 x n\n",
    "        c_t = c_t.view(-1, config.hidden_dim * 2)  # B x 2*hidden_dim\n",
    "\n",
    "        attn_dist = attn_dist.view(-1, t_k)  # B x t_k\n",
    "\n",
    "        if config.is_coverage:\n",
    "            coverage = coverage.view(-1, t_k)\n",
    "            coverage = coverage + attn_dist\n",
    "\n",
    "        return c_t, attn_dist, coverage\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention_network = Attention()\n",
    "        # decoder\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "\n",
    "        self.x_context = nn.Linear(config.hidden_dim * 2 + config.emb_dim, config.emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(config.emb_dim,\n",
    "                            config.hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "\n",
    "        if config.pointer_gen:\n",
    "            self.p_gen_linear = nn.Linear(config.hidden_dim * 4 + config.emb_dim, 1)\n",
    "\n",
    "        #p_vocab\n",
    "        self.out1 = nn.Linear(config.hidden_dim * 3, config.hidden_dim)\n",
    "        self.out2 = nn.Linear(config.hidden_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, y_t_1, s_t_1, encoder_outputs, encoder_feature, enc_padding_mask, c_t_1,\n",
    "                extra_zeros, enc_batch_extend_vocab, coverage, step):\n",
    "\n",
    "        if not self.training and step == 0:\n",
    "            h_decoder, c_decoder = s_t_1\n",
    "            s_t_hat = torch.cat(\n",
    "                (h_decoder.view(-1, config.hidden_dim), c_decoder.view(-1, config.hidden_dim)),\n",
    "                1)  # B x 2*hidden_dim\n",
    "            c_t, _, coverage_next = self.attention_network(s_t_hat, encoder_outputs,\n",
    "                                                           encoder_feature, enc_padding_mask,\n",
    "                                                           coverage)\n",
    "            coverage = coverage_next\n",
    "\n",
    "        y_t_1_embd = self.embedding(y_t_1)\n",
    "        x = self.x_context(torch.cat((c_t_1, y_t_1_embd), 1))\n",
    "        lstm_out, s_t = self.lstm(x.unsqueeze(1), s_t_1)\n",
    "\n",
    "        h_decoder, c_decoder = s_t\n",
    "        s_t_hat = torch.cat(\n",
    "            (h_decoder.view(-1, config.hidden_dim), c_decoder.view(-1, config.hidden_dim)),\n",
    "            1)  # B x 2*hidden_dim\n",
    "        c_t, attn_dist, coverage_next = self.attention_network(s_t_hat, encoder_outputs,\n",
    "                                                               encoder_feature, enc_padding_mask,\n",
    "                                                               coverage)\n",
    "\n",
    "        if self.training or step > 0:\n",
    "            coverage = coverage_next\n",
    "\n",
    "        p_gen = None\n",
    "        if config.pointer_gen:\n",
    "            p_gen_input = torch.cat((c_t, s_t_hat, x), 1)  # B x (2*2*hidden_dim + emb_dim)\n",
    "            p_gen = self.p_gen_linear(p_gen_input)\n",
    "            p_gen = F.sigmoid(p_gen)\n",
    "\n",
    "        output = torch.cat((lstm_out.view(-1, config.hidden_dim), c_t), 1)  # B x hidden_dim * 3\n",
    "        output = self.out1(output)  # B x hidden_dim\n",
    "\n",
    "        #output = F.relu(output)\n",
    "\n",
    "        output = self.out2(output)  # B x vocab_size\n",
    "        vocab_dist = F.softmax(output, dim=1)\n",
    "\n",
    "        if config.pointer_gen:\n",
    "            vocab_dist_ = p_gen * vocab_dist\n",
    "            attn_dist_ = (1 - p_gen) * attn_dist\n",
    "\n",
    "            if extra_zeros is not None:\n",
    "                vocab_dist_ = torch.cat([vocab_dist_, extra_zeros], 1)\n",
    "\n",
    "            final_dist = vocab_dist_.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
    "        else:\n",
    "            final_dist = vocab_dist\n",
    "\n",
    "        return final_dist, s_t, c_t, attn_dist, p_gen, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dicts = trainer.load_jsons('data/train1.json')\n",
    "examples = list(map(trainer.convert_article_to_example, article_dicts))\n",
    "\n",
    "data_loader = trainer.build_data_loader(examples)\n",
    "\n",
    "for batch in data_loader:\n",
    "    batch_size = batch.enc.input_ids.size(0)\n",
    "    enc_outputs, enc_feature, enc_hidden = enc(batch.enc.input_ids)\n",
    "    s_t_1 = reduce_state(enc_hidden)\n",
    "    c_t_1 = torch.zeros((batch_size, 2 * config.hidden_dim))\n",
    "    extra_zeros = torch.zeros((batch_size, batch.enc.max_n_oovs))\n",
    "    coverage = torch.zeros(batch.enc.input_ids.size())\n",
    "    for di in range(TGT_MAX_LENGTH):\n",
    "        y_t_1 = batch.dec.input_ids[:, di]  # Teacher forcing\n",
    "        final_dist, s_t_1, c_t_1, attn_dist, p_gen, next_coverage = dec(\n",
    "                y_t_1, s_t_1, enc_outputs, enc_feature, batch.enc.mask, c_t_1,\n",
    "                extra_zeros, batch.enc.extended_ids, coverage, di)\n",
    "        target = batch.dec.target_ids[:, di]\n",
    "        gold_probs = torch.gather(final_dist, 1, target.unsqueeze(1)).squeeze()\n",
    "        step_loss = -torch.log(gold_probs + 1e-12)\n",
    "\n",
    "        step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "        step_loss += step_coverage_loss\n",
    "        coverage = next_coverage\n",
    "\n",
    "        step_mask = batch.dec.mask[:, di]\n",
    "        step_loss = step_loss * step_mask\n",
    "\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 2 is out of bounds for dimension 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jq/05s996j133v2w4fgvj10llnh0000gn/T/ipykernel_22361/2888284912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# torch.gather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 2 is out of bounds for dimension 1 with size 2"
     ]
    }
   ],
   "source": [
    "true = torch.tensor([1, 0, 2])\n",
    "out = torch.softmax(torch.rand(3, 2), -1)\n",
    "torch.gather(out, 1, true.unsqueeze(1))\n",
    "# torch.gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d23c48c2e76b1d188bcde8d26c15818e88941b947077ebf6a883ea66e2e209df"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
